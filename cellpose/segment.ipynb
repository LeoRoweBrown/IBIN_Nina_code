{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "from tifffile import imread\n",
    "from tifffile import imwrite\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import napari\n",
    "import time\n",
    "from glob import glob\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# cellpose nuclei model is trained on diameter of 17, but our data is 13:\n",
    "# maybe nuclei model training data should be rescaled to have diameter of 13 and we train *everything* together\n",
    "# as suggested by \"https://cellpose.readthedocs.io/en/latest/models.html\"\n",
    "diam_mean = 13  \n",
    "\n",
    "# model location\n",
    "# trained on G:\\Data\\IBIN_Nina\\workspace\\nina_cellpose\\training_data\\with_reslice\\mask_only_right\n",
    "model_path = \"C:/Users/OPMuser/nina_cellpose/training_data/with_reslice/model/models/cellpose_residual_on_style_on_concatenation_off__2022_10_18_14_57_00.795536\"\n",
    "\n",
    "# save dir for segmentation masks\n",
    "save_dir = \"T:/IBIN_Nina/workspace/main_live4/trained_masks_new_model/all_runs_Hugh_test/\"\n",
    "\n",
    "# root dir of dataset with structure: <root>/run_001/main/YY-MM-DD/HH-MM-SS/cst4/run_xxx/field_xxx/excxxx_filterxxx\n",
    "root_dir = \"T:/IBIN_Nina/temp/20221125_live_plate4/\"\n",
    "\n",
    "search_dir = os.path.join(root_dir, \"**/cst4/run_*\")\n",
    "search_dir = os.path.join(root_dir, \"main/*/*/cst4/run_*\")\n",
    "\n",
    "print(search_dir)\n",
    "all_runs = glob(search_dir, recursive=True)\n",
    "print(all_runs)\n",
    "print(len(all_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"  ## attempts to prevent OOM\n",
    "\n",
    "diam_mean = 15\n",
    "\n",
    "search_dir = os.path.join(root_dir, \"main*/*/*/cst4/run_*\")\n",
    "print(search_dir)\n",
    "all_runs = glob(search_dir, recursive=True)\n",
    "print(all_runs)\n",
    "\n",
    "fields_to_include = None # [1, 5, 16, 49, 52, 64]  # if None, run all\n",
    "runs_to_include = None# [10,11]  # counted from zero, if None run all\n",
    "\n",
    "print(np.array(runs_to_include))\n",
    "if runs_to_include is not None:\n",
    "    runs_dirs = [all_runs[n] for n in runs_to_include]\n",
    "else:\n",
    "    runs_dirs = all_runs\n",
    "    runs_to_include = range(len(all_runs))\n",
    "\n",
    "f = open(\"process_details.txt\", \"a\")\n",
    "f.write(\"data path: \" + root_dir)\n",
    "f.write(\"model path: \" + model_path)\n",
    "f.write(\"diam_mean: %.1f\" % diam_mean)\n",
    "f.close()\n",
    "\n",
    "for n_i in range(len(all_runs)):\n",
    "    rundir = runs_dirs[n_i]\n",
    "    n = runs_to_include[n_i]  # n_i is just arb. index, n is actual run number \n",
    "    fields = glob(os.path.join(rundir, \"field_*\"))  # get list of field dirs (field_0001/ etc.)\n",
    "    print(rundir)\n",
    "    f = open(\"info.txt\", \"a\")\n",
    "    f.write(rundir)\n",
    "    f.close()\n",
    "    for f in range(len(fields)):\n",
    "        # field number number counts from 1, so do f+1 \n",
    "        if fields_to_include is not None and f+1 not in fields_to_include:\n",
    "            continue\n",
    "\n",
    "        save_dir_run = os.path.join(save_dir, \"run_%03d\" % (n+1))\n",
    "\n",
    "        if not os.path.exists(save_dir_run):\n",
    "            os.makedirs(save_dir_run)\n",
    "\n",
    "        fname = 'field%03d_trained.tif' % (f+1)\n",
    "        savepath = os.path.join(save_dir_run, fname)\n",
    "\n",
    "        if os.path.exists(savepath):\n",
    "            print(\"Segmented image\", savepath, \"exists, skipping\", n )\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        load_dir = os.path.join(fields[f], 'exc561_filter605')\n",
    "        seq = glob(os.path.join(load_dir, \"*.tif\"))\n",
    "        stack = imread(seq)\n",
    "\n",
    "\n",
    "        print(\"segmenting %s\" % fields[f])\n",
    "\n",
    "        ## remove bg/offset from data added by camera etc.\n",
    "        stack = np.asarray(stack, dtype=np.int32)\n",
    "        stack -= 100  # subtract from signed int\n",
    "        negative_mask = stack > 0\n",
    "        stack = stack*negative_mask  # remove negative values\n",
    "        stack = np.asarray(stack, dtype=np.uint16)  # convert back to unsigned\n",
    "\n",
    "        # should we initialise this less often\n",
    "        model = models.CellposeModel(pretrained_model = model_path,\n",
    "                                diam_mean = diam_mean,\n",
    "                                model_type=None,\n",
    "                                gpu=True,\n",
    "                                torch = True,\n",
    "                                net_avg = True\n",
    "                            )\n",
    "\n",
    "        output = model.eval(stack, channels=[0,0], do_3D=True, diameter=diam_mean)\n",
    "        masks = output[0]\n",
    "\n",
    "        segment_time = time.time() - start_time\n",
    "        print(\"Time taken to segment:\", segment_time, \"s\")\n",
    "\n",
    "        ## save\n",
    "        print(\"saving:\", savepath)\n",
    "        imwrite(savepath, masks)\n",
    "print(\"finished on\", n, rundir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
